ğŸ“§ Email Spam or Ham Classification

This repository contains the implementation of Experiment 3 for the Machine Learning Algorithms Laboratory (ICS1512).
The goal is to classify emails as Spam or Ham using Naive Bayes, KNN, and SVM and compare their performance.

ğŸ“Œ Objectives

Preprocess email text data (cleaning, vectorization, train-test split).

Implement:

Naive Bayes variants: BernoulliNB, MultinomialNB, GaussianNB

K-Nearest Neighbors (KNN) with different k values and tree structures

Support Vector Machine (SVM) with multiple kernels (linear, polynomial, RBF, sigmoid)

Compare models using:

    Accuracy

    Precision

    Recall

    F1 Score

    Confusion Matrix

    ROC-AUC

Perform Cross-validation for model robustness.

ğŸ“š Libraries Used

    Python 3.x

    Numpy

    Pandas

    Scikit-learn

    Matplotlib

    Seaborn

ğŸ“Š Results

ğŸ”¹ Naive Bayes Variants

| Model         | Accuracy | Precision | Recall | F1 Score |
| ------------- | -------- | --------- | ------ | -------- |
| BernoulliNB   | 0.8899   | 0.8843    | 0.8290 | 0.8558   |
| MultinomialNB | 0.8950   | 0.9424    | 0.7813 | 0.8543   |
| GaussianNB    | 0.8197   | 0.7001    | 0.9485 | 0.8056   |

ğŸ”¹ KNN (different k values)

Best performance at k=5 with Accuracy = 0.8993, F1 Score = 0.8705.

ğŸ”¹ SVM Kernels

RBF Kernel performed best with Accuracy â‰ˆ 92.6% and F1 Score â‰ˆ 0.90.

ğŸ“ Observations

    Naive Bayes: Simple, efficient, works well with text data.

    KNN: High accuracy at low k but computationally heavier.

    SVM (RBF kernel): Best overall performance, good for non-linear decision boundaries.

âœ… Conclusion

Best Model: SVM with RBF kernel (Accuracy â‰ˆ 92.7%).

Trade-offs:

    Naive Bayes â†’ Fast, scalable.

    KNN â†’ High accuracy but slower for large datasets.

    SVM â†’ Balanced choice (accuracy + robustness).
