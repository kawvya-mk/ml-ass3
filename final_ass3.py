# -*- coding: utf-8 -*-
"""final_ass3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzU9JsBfKyOeqwTH2iCQc6UcaDqK8Nzr
"""

#IMPORTS
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc
)

# --------------------------
# 1. Load Dataset
# --------------------------
from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv('/content/drive/MyDrive/spambase_csv.csv')
print("Shape:", df.shape)
print(df.head())

# --------------------------
# 2. Check & handle missing
# --------------------------
print("\nMissing values:", df.isna().sum().sum())
df = df.dropna()

# --------------------------
# 3. Separate features & label
# --------------------------
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# --------------------------
# 4. Scale features
# --------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --------------------------
# 5. Train-Test split
# --------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

# --------------------------
# Utility functions
# --------------------------
def evaluate_model(model, X_train, X_test, y_train, y_test):
    start = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start
    y_pred = model.predict(X_test)
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1 Score': f1_score(y_test, y_pred),
        'Train Time (s)': train_time
    }
    return model, y_pred, metrics

def plot_confusion(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {title}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

def plot_roc(model, X_test, y_test, title):
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:,1]
    else:
        y_score = model.decision_function(X_test)
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{title} (AUC={roc_auc:.2f})')
    plt.plot([0,1], [0,1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {title}')
    plt.legend()
    plt.show()

# --------------------------
# 6. Naive Bayes Variants
# --------------------------
nb_models = {
    'GaussianNB': Pipeline([('scaler', StandardScaler()), ('clf', GaussianNB())]),
    'MultinomialNB': Pipeline([('minmax', MinMaxScaler()), ('clf', MultinomialNB())]),
    'BernoulliNB': Pipeline([('binarizer', Binarizer(threshold=0.0)), ('clf', BernoulliNB())])
}

nb_results = {}
for name, model in nb_models.items():
    m, y_pred, metrics = evaluate_model(model, X_train, X_test, y_train, y_test)
    nb_results[name] = metrics
    print(f"\n{name} Report:\n", classification_report(y_test, y_pred))
    plot_confusion(y_test, y_pred, name)
    plot_roc(m, X_test, y_test, name)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print(accuracy_score(y_test, y_pred))
print(precision_score(y_test, y_pred))
print(recall_score(y_test, y_pred))
print(f1_score(y_test, y_pred))

nb_df = pd.DataFrame(nb_results).T
print("\nNaive Bayes Comparison:")
print(nb_df)

# --------------------------
# 7. KNN - different k values
# --------------------------
k_values = [1, 3, 5, 7]
knn_results = {}
for k in k_values:
    knn = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier(n_neighbors=k))])
    m, y_pred, metrics = evaluate_model(knn, X_train, X_test, y_train, y_test)
    knn_results[f'k={k}'] = metrics
    plot_confusion(y_test, y_pred, f'KNN (k={k})')

knn_df = pd.DataFrame(knn_results).T
print("\nKNN (varying k) Comparison:")
print(knn_df)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print(accuracy_score(y_test, y_pred))
print(precision_score(y_test, y_pred))
print(recall_score(y_test, y_pred))
print(f1_score(y_test, y_pred))

# --------------------------
# 8. KNN - KDTree vs BallTree
# --------------------------
tree_results = {}
for algo in ['kd_tree', 'ball_tree']:
    knn = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier(n_neighbors=5, algorithm=algo))])
    m, y_pred, metrics = evaluate_model(knn, X_train, X_test, y_train, y_test)
    tree_results[algo] = metrics
    plot_confusion(y_test, y_pred, f'KNN ({algo})')

tree_df = pd.DataFrame(tree_results).T
print("\nKNN Tree Comparison:")
print(tree_df)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print(accuracy_score(y_test, y_pred))
print(precision_score(y_test, y_pred))
print(recall_score(y_test, y_pred))
print(f1_score(y_test, y_pred))

# --------------------------
# 9. SVM - kernels
# --------------------------
svm_params = {
    'Linear': {'kernel':'linear', 'C':1.0, 'probability':True},
    'Polynomial': {'kernel':'poly', 'degree':3, 'C':1.0, 'gamma':'scale', 'probability':True},
    'RBF': {'kernel':'rbf', 'C':1.0, 'gamma':'scale', 'probability':True},
    'Sigmoid': {'kernel':'sigmoid', 'C':1.0, 'gamma':'scale', 'probability':True}
}

svm_results = {}
for name, params in svm_params.items():
    svm = Pipeline([('scaler', StandardScaler()), ('clf', SVC(**params))])
    m, y_pred, metrics = evaluate_model(svm, X_train, X_test, y_train, y_test)
    svm_results[name] = metrics
    plot_confusion(y_test, y_pred, f'SVM ({name})')
    plot_roc(m, X_test, y_test, f'SVM ({name})')

svm_df = pd.DataFrame(svm_results).T
print("\nSVM Kernel Comparison:")
print(svm_df)

# --------------------------
# 10. K-Fold Cross Validation
# --------------------------
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_results = {}
models_for_cv = {
    'GaussianNB': nb_models['GaussianNB'],
    'KNN(k=5)': Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier(n_neighbors=5))]),
    'SVM(Linear)': Pipeline([('scaler', StandardScaler()), ('clf', SVC(kernel='linear'))])
}

for name, model in models_for_cv.items():
    scores = cross_val_score(model, X_scaled, y, cv=kf, scoring='accuracy')
    cv_results[name] = scores

cv_df = pd.DataFrame(cv_results)
print("\nK-Fold Cross Validation Results:")
print(cv_df)
print("\nAverage CV Accuracy:")
print(cv_df.mean())